# Awesome VLA for Robotic Manipulation

<div style="text-align: center;">
  <img src="./pipline & timeline.png" alt="image info">
</div>


üî• Vision-Language-Action (VLA) models have recently emerged as a transformative paradigm for robotic manipulation by tightly coupling perception, language understanding, and action generation. Built upon large-scale Vision-Language Models (VLMs), they enable robots to interpret natural language instructions, perceive complex environments, and perform diverse manipulation tasks with strong generalization.    

üìç We present **the first systematic survey on VLM-based VLA models for robotic manipulation**. This repository serves as the companion resource to our survey: ["Vision-Language-Action Models for Robotic Manipulation: A Survey"](https://arxiv.org/abs/xxxx.xxxxx), and includes all the research papers, benchmarks, and resources reviewed in the paper, organized for easy access and reference.

üìå We will keep updating this repository with newly published works to reflect the latest progress in the field. 


## Table of Contents

- [ü§ñ Awesome VLA for Robotic Manipulation](#awesome-vla-for-robotic-manipulation)
  - [üîç Table of Contents](#table-of-contents)
  - [üóÇÔ∏è Datasets and Benchmarks](#datasets-and-benchmarks)
    - [Real-world Robot Datasets](#real-world-robot-datasets)
    - [Simulation Environments and Benchmarks](#simulation-environments-and-benchmarks)
    - [Human Behavior Datasets](#human-behavior-datasets)
    - [Embodied Datasets and Benchmarks](#embodied-datasets-and-benchmarks)
  - [üßæ Monolithic Models](#monolithic-models)
    - [Single-System](#single-system)
    - [Dual-System](#dual-system)
  - [üß© Hierarchical Models](#hierarchical-models)
    - [Planner Only](#planner-only)
    - [Planner + Policy](#planner--policy)
  - [üöÄ Other Advanced Field](#other-advanced-field)
    - [Reinforcement Learning-based Methods](#reinforcement-learning-based-methods)
    - [Training-Free Methods](#training-free-methods)
    - [Learning from Human Videos](#learning-from-human-videos)
    - [World Model-based VLA](#world-model-based-vla)


## Datasets and Benchmarks

### Real-world Robot Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2021 | CoRL  | [Bc-z: Zero-shot task generalization with robotic imitation learning](https://proceedings.mlr.press/v164/jang22a/jang22a.) | [üåê](https://sites.google.com/view/bc-z/home) | [üíª](https://github.com/google-research/tensor2robot/tree/master/research/bcz) | [üì¶](https://www.kaggle.com/datasets/google/bc-z-robot) |

---

### Simulation Environments and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|

---

### Human Behavior Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|

---

### Embodied Datasets and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|







## Monolithic Models

### Single-System
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Dual-System
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|





## Hierarchical Models

### Planner Only
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Planner + Policy
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|







## Other Advanced Field

### Reinforcement Learning-based Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Training-Free Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Learning from Human Videos
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### World Model-based VLA
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
