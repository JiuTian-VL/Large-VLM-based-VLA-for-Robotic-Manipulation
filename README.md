# Awesome VLA for Robotic Manipulation

<div style="text-align: center;">
  <img src="./pipline & timeline.png" alt="image info">
</div>


ğŸ”¥ Vision-Language-Action (VLA) models have recently emerged as a transformative paradigm for robotic manipulation by tightly coupling perception, language understanding, and action generation. Built upon large-scale Vision-Language Models (VLMs), they enable robots to interpret natural language instructions, perceive complex environments, and perform diverse manipulation tasks with strong generalization.    

ğŸ“ We present **the first systematic survey on VLM-based VLA models for robotic manipulation**. This repository serves as the companion resource to our survey: ["Vision-Language-Action Models for Robotic Manipulation: A Survey"](https://arxiv.org/abs/xxxx.xxxxx), and includes all the research papers, benchmarks, and resources reviewed in the paper, organized for easy access and reference.

ğŸ“Œ We will keep updating this repository with newly published works to reflect the latest progress in the field. 


## Table of Contents

- [ğŸ¤– Awesome VLA for Robotic Manipulation](#awesome-vla-for-robotic-manipulation)
  - [ğŸ” Table of Contents](#table-of-contents)
  - [ğŸ—‚ï¸ Datasets and Benchmarks](#datasets-and-benchmarks)
    - [Real-world Robot Datasets](#real-world-robot-datasets)
    - [Simulation Environments and Benchmarks](#simulation-environments-and-benchmarks)
    - [Human Behavior Datasets](#human-behavior-datasets)
    - [Embodied Datasets and Benchmarks](#embodied-datasets-and-benchmarks)
  - [ğŸ§¾ Monolithic Models](#monolithic-models)
    - [Single-System](#single-system)
    - [Dual-System](#dual-system)
  - [ğŸ§© Hierarchical Models](#hierarchical-models)
    - [Planner Only](#planner-only)
    - [Planner + Policy](#planner--policy)
  - [ğŸš€ Other Advanced Field](#other-advanced-field)
    - [Reinforcement Learning-based Methods](#reinforcement-learning-based-methods)
    - [Training-Free Methods](#training-free-methods)
    - [Learning from Human Videos](#learning-from-human-videos)
    - [World Model-based VLA](#world-model-based-vla)


## Datasets and Benchmarks

### Real-world Robot Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2021 | CoRL  | [Bc-z: Zero-shot task generalization with robotic imitation learning](https://proceedings.mlr.press/v164/jang22a/jang22a.) | [ğŸŒ](https://sites.google.com/view/bc-z/home) | [ğŸ’»](https://github.com/google-research/tensor2robot/tree/master/research/bcz) | [ğŸ“¦](https://www.kaggle.com/datasets/google/bc-z-robot) |

---

### Simulation Environments and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|

---

### Human Behavior Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|

---

### Embodied Datasets and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|







## Monolithic Models

### Single-System
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Dual-System
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|





## Hierarchical Models

### Planner Only
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Planner + Policy
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|







## Other Advanced Field

### Reinforcement Learning-based Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2025 | ICLR(Workshop)  | [GRAPE: Generalizing Robot Policy via Preference Alignment](https://openreview.net/pdf?id=XnwyFD1Fvw&utm_source=chatgpt.com) | [ğŸŒ](https://grape-vla.github.io/) | [ğŸ’»](https://github.com/aiming-lab/grape) |
| 2025 | arXiv | [Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning](https://arxiv.org/abs/2505.18719) | - | [ğŸ’»](https://github.com/GuanxingLu/vlarl) |
| 2025 | RSS | [ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations](https://openreview.net/pdf?id=a6lsCozWyM) | [ğŸŒ](https://rewind-reward.github.io/) | - |
| 2025 | RSS | [ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy](https://www.roboticsproceedings.org/rss21/p019.pdf) | [ğŸŒ](https://cccedric.github.io/conrft/) | [ğŸ’»](https://github.com/cccedric/conrft) |
| 2025| arXiv | [TGRPO: Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization](https://arxiv.org/abs/2506.08440) | - | - |
---

### Training-Free Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### Learning from Human Videos
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|

---

### World Model-based VLA
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
