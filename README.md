<div align="center">

# Awesome VLA for Robotic Manipulation

<p align="center">
  <big><big><big><strong>â­ Give us a star if you like it! â­</strong></big></big><big>
</p>

<p align="center">
  <img src="./pipline & timeline.png" alt="image info">
</p>

</div>


ğŸ”¥ Large VLM-based Vision-Language-Action (VLA) models have recently emerged as a transformative paradigm for robotic manipulation by tightly coupling perception, language understanding, and action generation. Built upon large Vision-Language Models (VLMs), they enable robots to interpret natural language instructions, perceive complex environments, and perform diverse manipulation tasks with strong generalization.    

ğŸ“ We present **the first systematic survey on large VLM-based VLA models for robotic manipulation**. This repository serves as the companion resource to our survey: ["Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey"](https://arxiv.org/pdf/2508.13073), and includes all the research papers, benchmarks, and resources reviewed in the paper, organized for easy access and reference.

ğŸ“Œ We will keep updating this repository with newly published works to reflect the latest progress in the field. 


## Table of Contents

- [ğŸ¤– Awesome VLA for Robotic Manipulation](#awesome-vla-for-robotic-manipulation)
  - [ğŸ” Table of Contents](#table-of-contents)
  - [ğŸ§¾ Monolithic Models](#monolithic-models)
    - [Single-System](#single-system)
    - [Dual-System](#dual-system)
  - [ğŸ§© Hierarchical Models](#hierarchical-models)
    - [Planner Only](#planner-only)
    - [Planner + Policy](#planner--policy)
  - [ğŸš€ Other Advanced Field](#other-advanced-field)
    - [Reinforcement Learning-based Methods](#reinforcement-learning-based-methods)
    - [Training-Free Methods](#training-free-methods)
    - [Learning from Human Videos](#learning-from-human-videos)
    - [World Model-based VLA](#world-model-based-vla)
  - [ğŸ—‚ï¸ Datasets and Benchmarks](#datasets-and-benchmarks)
    - [Real-world Robot Datasets](#real-world-robot-datasets)
    - [Simulation Environments and Benchmarks](#simulation-environments-and-benchmarks)
    - [Human Behavior Datasets](#human-behavior-datasets)
    - [Embodied Datasets and Benchmarks](#embodied-datasets-and-benchmarks)


## Monolithic Models

### Single-System
<!-- | Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2023 | CoRL | [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818) | [ğŸŒ](https://robotics-transformer2.github.io/) | [ğŸ’»](-) |
| 2023 | ICRA | [RT-2-X: Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864) | [ğŸŒ](https://robotics-transformer-x.github.io/) | [ğŸ’»](-) |
| 2023 | NeurIPS | [RoboFlamingo: Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2311.01378) | [ğŸŒ](https://roboflamingo.github.io/) | [ğŸ’»](https://roboflamingo.github.io/) |
| 2023 | ICML | [LEO Agent: An Embodied Generalist Agent in 3D World](https://arxiv.org/abs/2311.12871) | [ğŸŒ](https://embodied-generalist.github.io/) | [ğŸ’»](https://github.com/embodied-generalist/embodied-generalist) |
| 2024 | NeurIPS | [RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation](https://arxiv.org/abs/2406.04339) | [ğŸŒ](https://sites.google.com/view/robomamba-web) | [ğŸ’»](https://github.com/lmzpai/roboMamba?tab=readme-ov-file) |
| 2024 | CoRL | [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246) | [ğŸŒ](https://openvla.github.io/) | [ğŸ’»](https://github.com/openvla/openvla?tab=readme-ov-file) |
| 2024 | CoRL | [ECOT-Lite: Robotic Control via Embodied Chain-of-Thought Reasoning](https://arxiv.org/abs/2407.08693) | [ğŸŒ](https://embodied-cot.github.io/) | [ğŸ’»](https://github.com/MichalZawalski/embodied-CoT/) |
| 2024 | ICRA | [ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](https://arxiv.org/abs/2409.15250) | [ğŸŒ](https://insait-institute.github.io/ReVLA/) | [ğŸ’»](-) |
| 2024 | NeurIPS | [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | [ğŸŒ](-) | [ğŸ’»](https://github.com/yueyang130/DeeR-VLA) |
| 2024 | ICLR | [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://arxiv.org/abs/2412.10345) | [ğŸŒ](https://tracevla.github.io/) | [ğŸ’»](https://github.com/umd-huang-lab/tracevla) |
| 2025 | ICRA | [FuSe: Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding](https://arxiv.org/abs/2501.04693) | [ğŸŒ](https://fuse-model.github.io/) | [ğŸ’»](https://github.com/fuse-model/FuSe) |
| 2025 | CVPR | [UniAct: Universal Actions for Enhanced Embodied Foundation Models](https://arxiv.org/abs/2501.10105) | [ğŸŒ](https://2toinf.github.io/UniAct/) | [ğŸ’»](https://github.com/2toinf/UniAct) |
| 2025 | arXiv | [SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model](https://arxiv.org/abs/2501.15830) | [ğŸŒ](https://spatialvla.github.io/) | [ğŸ’»](https://github.com/SpatialVLA/SpatialVLA) |
| 2025 | ICML | [UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent](https://arxiv.org/abs/2501.18867) | [ğŸŒ](-) | [ğŸ’»](https://github.com/CladernyJorn/UP-VLA) |
| 2025 | ICLR | [VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation](https://arxiv.org/abs/2502.13508) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [OpenVLA-OFT: Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645) | [ğŸŒ](https://openvla-oft.github.io/) | [ğŸ’»](https://github.com/moojink/openvla-oft) |
| 2025 | arXiv | [PD-VLA: Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/abs/2503.02310) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](https://arxiv.org/abs/2503.10631) | [ğŸŒ](https://hybrid-vla.github.io/) | [ğŸ’»](https://github.com/PKU-HMI-Lab/Hybrid-VLA) |
| 2025 | arXiv | [MoLe-VLA: Dynamic Layer-Skipping Vision-Language-Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384) | [ğŸŒ](https://sites.google.com/view/mole-vla) | [ğŸ’»](https://github.com/RoyZry98/MoLe-VLA-Pytorch) |
| 2025 | CVPR | [CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](https://arxiv.org/abs/2503.22020) | [ğŸŒ](https://cot-vla.github.io/) | [ğŸ’»](-) |
| 2025 | arXiv | [NORA: A Small Open-Sourced Generalist Vision-Language-Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854) | [ğŸŒ](https://declare-lab.github.io/nora) | [ğŸ’»](https://github.com/declare-lab/nora) |
| 2025 | arXiv | [VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation](https://arxiv.org/abs/2505.09577) | [ğŸŒ](https://sites.google.com/view/vtla) | [ğŸ’»](-) |
| 2025 | arXiv | [OE-VLA: Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions](https://arxiv.org/abs/2505.11214) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning](https://arxiv.org/abs/2505.19080) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [FLashVLA: Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models](https://arxiv.org/abs/2505.21200) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks](https://arxiv.org/abs/2506.00411) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [BitVLA: 1-Bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530) | [ğŸŒ](-) | [ğŸ’»](https://github.com/ustcwhy/BitVLA?tab=readme-ov-file) |
| 2025 | arXiv | [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961) | [ğŸŒ](https://bridgevla.github.io/home_page.html) | [ğŸ’»](https://github.com/BridgeVLA/BridgeVLA) |
| 2025 | arXiv | [UniVLA: Unified Vision-Language-Action Model](https://arxiv.org/abs/2506.17142) | [ğŸŒ](https://robertwyq.github.io/univla.github.io/) | [ğŸ’»](https://github.com/baaivision/UniVLA) |
| 2025 | arXiv | [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539) | [ğŸŒ](-) | [ğŸ’»](https://github.com/alibaba-damo-academy/WorldVLA) |
| 2025 | arXiv | [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242) | [ğŸŒ](-) | [ğŸ’»](https://github.com/fudan-zvg/4D-VLA) |
| 2025 | ICCV | [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/abs/2507.01016) | [ğŸŒ](https://xiaoxiao0406.github.io/vqvla.github.io/) | [ğŸ’»](https://github.com/xiaoxiao0406/VQ-VLA) |
| 2025 | arXiv | [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [SpecVLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424) | [ğŸŒ](-) | [ğŸ’»](-) |
| 2025 | arXiv | [ST-VLA: Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding](https://arxiv.org/abs/2508.09032) | [ğŸŒ](https://ampiromax.github.io/ST-VLA/) | [ğŸ’»](-) | -->


| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2023 | CoRL | [RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control](https://arxiv.org/abs/2307.15818) | [ğŸŒ](https://robotics-transformer2.github.io/) | - |
| 2023 | ICRA | [RT-2-X: Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864) | [ğŸŒ](https://robotics-transformer-x.github.io/) | - |
| 2023 | NeurIPS | [RoboFlamingo: Vision-Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2311.01378) | [ğŸŒ](https://roboflamingo.github.io/) | [ğŸ’»](https://roboflamingo.github.io/) |
| 2023 | ICML | [LEO Agent: An Embodied Generalist Agent in 3D World](https://arxiv.org/abs/2311.12871) | [ğŸŒ](https://embodied-generalist.github.io/) | [ğŸ’»](https://github.com/embodied-generalist/embodied-generalist) |
| 2024 | NeurIPS | [RoboMamba: Efficient Vision-Language-Action Model for Robotic Reasoning and Manipulation](https://arxiv.org/abs/2406.04339) | [ğŸŒ](https://sites.google.com/view/robomamba-web) | [ğŸ’»](https://github.com/lmzpai/roboMamba?tab=readme-ov-file) |
| 2024 | CoRL | [OpenVLA: An Open-Source Vision-Language-Action Model](https://arxiv.org/abs/2406.09246) | [ğŸŒ](https://openvla.github.io/) | [ğŸ’»](https://github.com/openvla/openvla?tab=readme-ov-file) |
| 2024 | CoRL | [ECOT-Lite: Robotic Control via Embodied Chain-of-Thought Reasoning](https://arxiv.org/abs/2407.08693) | [ğŸŒ](https://embodied-cot.github.io/) | [ğŸ’»](https://github.com/MichalZawalski/embodied-CoT/) |
| 2024 | ICRA | [ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models](https://arxiv.org/abs/2409.15250) | [ğŸŒ](https://insait-institute.github.io/ReVLA/) | - |
| 2024 | NeurIPS | [DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution](https://arxiv.org/abs/2411.02359) | - | [ğŸ’»](https://github.com/yueyang130/DeeR-VLA) |
| 2024 | ICLR | [TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies](https://arxiv.org/abs/2412.10345) | [ğŸŒ](https://tracevla.github.io/) | [ğŸ’»](https://github.com/umd-huang-lab/tracevla) |
| 2025 | ICRA | [FuSe: Beyond Sight: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding](https://arxiv.org/abs/2501.04693) | [ğŸŒ](https://fuse-model.github.io/) | [ğŸ’»](https://github.com/fuse-model/FuSe) |
| 2025 | CVPR | [UniAct: Universal Actions for Enhanced Embodied Foundation Models](https://arxiv.org/abs/2501.10105) | [ğŸŒ](https://2toinf.github.io/UniAct/) | [ğŸ’»](https://github.com/2toinf/UniAct) |
| 2025 | arXiv | [SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model](https://arxiv.org/abs/2501.15830) | [ğŸŒ](https://spatialvla.github.io/) | [ğŸ’»](https://github.com/SpatialVLA/SpatialVLA) |
| 2025 | ICML | [UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent](https://arxiv.org/abs/2501.18867) | - | [ğŸ’»](https://github.com/CladernyJorn/UP-VLA) |
| 2025 | ICLR | [VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation](https://arxiv.org/abs/2502.13508) | - | - |
| 2025 | arXiv | [OpenVLA-OFT: Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success](https://arxiv.org/abs/2502.19645) | [ğŸŒ](https://openvla-oft.github.io/) | [ğŸ’»](https://github.com/moojink/openvla-oft) |
| 2025 | arXiv | [PD-VLA: Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/abs/2503.02310) | - | - |
| 2025 | arXiv | [HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model](https://arxiv.org/abs/2503.10631) | [ğŸŒ](https://hybrid-vla.github.io/) | [ğŸ’»](https://github.com/PKU-HMI-Lab/Hybrid-VLA) |
| 2025 | arXiv | [MoLe-VLA: Dynamic Layer-Skipping Vision-Language-Action Model via Mixture-of-Layers for Efficient Robot Manipulation](https://arxiv.org/abs/2503.20384) | [ğŸŒ](https://sites.google.com/view/mole-vla) | [ğŸ’»](https://github.com/RoyZry98/MoLe-VLA-Pytorch) |
| 2025 | CVPR | [CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models](https://arxiv.org/abs/2503.22020) | [ğŸŒ](https://cot-vla.github.io/) | - |
| 2025 | arXiv | [NORA: A Small Open-Sourced Generalist Vision-Language-Action Model for Embodied Tasks](https://arxiv.org/abs/2504.19854) | [ğŸŒ](https://declare-lab.github.io/nora) | [ğŸ’»](https://github.com/declare-lab/nora) |
| 2025 | arXiv | [VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation](https://arxiv.org/abs/2505.09577) | [ğŸŒ](https://sites.google.com/view/vtla) | - |
| 2025 | arXiv | [OE-VLA: Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions](https://arxiv.org/abs/2505.11214) | - | - |
| 2025 | arXiv | [ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning](https://arxiv.org/abs/2505.19080) | - | - |
| 2025 | arXiv | [FLashVLA: Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models](https://arxiv.org/abs/2505.21200) | - | - |
| 2025 | arXiv | [LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks](https://arxiv.org/abs/2506.00411) | - | - |
| 2025 | arXiv | [BitVLA: 1-Bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530) | - | [ğŸ’»](https://github.com/ustcwhy/BitVLA?tab=readme-ov-file) |
| 2025 | arXiv | [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961) | [ğŸŒ](https://bridgevla.github.io/home_page.html) | [ğŸ’»](https://github.com/BridgeVLA/BridgeVLA) |
| 2025 | arXiv | [UniVLA: Unified Vision-Language-Action Model](https://arxiv.org/abs/2506.17142) | [ğŸŒ](https://robertwyq.github.io/univla.github.io/) | [ğŸ’»](https://github.com/baaivision/UniVLA) |
| 2025 | arXiv | [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539) | - | [ğŸ’»](https://github.com/alibaba-damo-academy/WorldVLA) |
| 2025 | arXiv | [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242) | - | [ğŸ’»](https://github.com/fudan-zvg/4D-VLA) |
| 2025 | ICCV | [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/abs/2507.01016) | [ğŸŒ](https://xiaoxiao0406.github.io/vqvla.github.io/) | [ğŸ’»](https://github.com/xiaoxiao0406/VQ-VLA) |
| 2025 | arXiv | [VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting](https://arxiv.org/abs/2507.05116) | - | - |
| 2025 | arXiv | [SpecVLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424) | - | - |
| 2025 | arXiv | [ST-VLA: Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding](https://arxiv.org/abs/2508.09032) | [ğŸŒ](https://ampiromax.github.io/ST-VLA/) | - |


---

### Dual-System
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2024 | arXiv | [A dual process vla: efficient robotic manipulation leveraging vlm](https://arxiv.org/abs/2410.15549) | - | - |
| 2024 | arXiv | [Towards synergistic, generalized, and efficient dual-system for robotic manipulation](https://arxiv.org/abs/2410.08001) | [ğŸŒ](https://robodual.github.io/) | - |
| 2024 | IROS | [From llms to actions: latent codes as bridges in hierarchical robot control](https://arxiv.org/abs/2405.04798) | - | - |
| 2025 | arXiv | [Gr00t n1: an open foundation model for generalist humanoid robots](https://arxiv.org/abs/2503.14734) | - | - |
| 2024 | arXiv | [Cogact: a foundational vision-language-action model for synergizing cognition and action in robotic manipulation](https://arxiv.org/abs/2411.19650) | [ğŸŒ](https://cogact.github.io/) | [ğŸ’»](https://github.com/microsoft/CogACT) |
| 2025 | CoRL | [Hirt: enhancing robotic control with hierarchical robot transformers](https://arxiv.org/abs/2410.05273) | - | - |
| 2025 | arXiv | [Fast-in-slow: a dual-system foundation model unifying fast manipulation within slow reasoning](https://arxiv.org/abs/2506.01953) | [ğŸŒ](https://fast-in-slow.github.io/) | [ğŸ’»](https://github.com/CHEN-H01/Fast-in-Slow) |
| 2025 | arXiv | [Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation](https://arxiv.org/abs/2505.03912) | [ğŸŒ](https://openhelix-robot.github.io/) | [ğŸ’»](https://github.com/OpenHelix-robot/OpenHelix) |
| 2025 | arXiv | [Chatvla: unified multimodal understanding and robot control with vision-language-action model](https://arxiv.org/abs/2502.14420) | [ğŸŒ](https://chatvla.github.io/) | [ğŸ’»](https://github.com/tutujingyugang1/ChatVLA_public) |
| 2025 | arXiv | [Chatvla-2: vision-language-action model with open-world embodied reasoning from pretrained knowledge](https://arxiv.org/abs/2505.21906) | [ğŸŒ](https://chatvla-2.github.io/) | - |
| 2025 | ICML | [Diffusionvla: scaling robot foundation models via unified diffusion and autoregression](https://arxiv.org/abs/2412.03293v1) | [ğŸŒ](https://diffusion-vla.github.io/) | [ğŸ’»](https://github.com/juruobenruo/DexVLA) |
| 2025 | arXiv | [Trivla: a unified triple-system-based unified vision-language-action model for general robot control](https://arxiv.org/abs/2507.01424) | [ğŸŒ](https://zhenyangliu.github.io/TriVLA/) | - |
| 2025 | arXiv | [Information-theoretic graph fusion with vision-language-action model for policy reasoning and dual robotic control](https://arxiv.org/abs/2508.05342) | - | - |
| 2025 | arXiv | [Rationalvla: a rational vision-language-action model with dual system](https://arxiv.org/abs/2506.10826) | [ğŸŒ](https://irpn-eai.github.io/RationalVLA/) | - |
| 2025 | ICCV | [Vq-vla: improving vision-language-action models via scaling vector-quantized action tokenizers](https://arxiv.org/abs/2507.01016) | [ğŸŒ](https://xiaoxiao0406.github.io/vqvla.github.io/) | [ğŸ’»](https://github.com/xiaoxiao0406/VQ-VLA) |
| 2025 | RA-L | [Tinyvla: towards fast, data-efficient vision-language-action models for robotic manipulation](https://arxiv.org/abs/2409.12514) | [ğŸŒ](https://tiny-vla.github.io/) | [ğŸ’»](https://github.com/liyaxuanliyaxuan/TinyVLA) |
| 2025 | RSS | [Ï€0: A vision-language-action flow model for general robot control](https://arxiv.org/abs/2410.24164) | [ğŸŒ](https://www.physicalintelligence.company/blog/pi0) | - |
| 2025 | RSS | [Fast: efficient action tokenization for vision-language-action models](https://arxiv.org/abs/2501.09747) | [ğŸŒ](https://www.pi.website/research/fast) | - |
| 2025 | arXiv | [Ï€0.5: a vision language-action model with open-world generalization](https://arxiv.org/abs/2504.16054) | [ğŸŒ](https://www.pi.website/blog/pi05) | - |
| 2025 | arXiv | [Knowledge insulating vision-language-action models: train fast, run fast, generalize better](https://arxiv.org/abs/2505.23705) | [ğŸŒ](https://www.pi.website/research/knowledge_insulation) | - |
| 2025 | arXiv | [Forcevla: enhancing vla models with a force-aware moe for contact-rich manipulation](https://arxiv.org/abs/2505.22159) | [ğŸŒ](https://sites.google.com/view/forcevla2025) | - |
| 2025 | arXiv | [Smolvla: a vision-language-action model for affordable and efficient robotics](https://arxiv.org/abs/2506.01844) | - | [ğŸ’»](https://github.com/huggingface/lerobot) |
| 2025 | arXiv | [Onetwovla: a unified vision-language-action model with adaptive reasoning](https://arxiv.org/abs/2505.11917) | [ğŸŒ](https://one-two-vla.github.io/) | [ğŸ’»](https://github.com/Fanqi-Lin/OneTwoVLA) |
| 2025 | arXiv | [Tactile-vla: unlocking vision-language-action modelâ€™s physical knowledge for tactile generalization](https://arxiv.org/abs/2507.09160) | - | - |
| 2025 | arXiv | [Gr-3 technical report](https://arxiv.org/abs/2507.15493) | [ğŸŒ](https://seed.bytedance.com/en/GR3/) | - |
| 2025 | arXiv | [Villa-x: enhancing latent action modeling in vision-language-action models](https://arxiv.org/abs/2507.23682) | [ğŸŒ](https://microsoft.github.io/villa-x/) | [ğŸ’»](https://github.com/microsoft/villa-x/) |


## Hierarchical Models

### Planner Only
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2023 | ICML | [PaLM-E: An embodied multimodal language model](https://proceedings.mlr.press/v202/driess23a/driess23a.pdf) | [ğŸŒ](https://palm-e.github.io/) | - |
| 2023 | arXiv | [ViLa: Look before you leap: Unveiling the power of GPT-4V in robotic vision-language planning](https://arxiv.org/abs/2311.17842) | [ğŸŒ](https://robot-vila.github.io/) | - |
| 2024 | CVPR | [MoManipVLA: Transferring vision-language-action models for general mobile manipulation](https://openaccess.thecvf.com/content/CVPR2025/papers/Wu_MoManipVLA_Transferring_Vision-language-action_Models_for_General_Mobile_Manipulation_CVPR_2025_paper.pdf) | [ğŸŒ](https://gary3410.github.io/momanipVLA/) | - |
| 2024 | CoRL | [RoboPoint: A vision-language model for spatial affordance prediction in robotics](https://proceedings.mlr.press/v270/yuan25c.html) | [ğŸŒ](https://robo-point.github.io/) | [ğŸ’»](https://github.com/wentaoyuan/RoboPoint) |
| 2025 | arXiv | [ManipLVM-R1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models](https://arxiv.org/abs/2505.16517) | - | - |
| 2025 | arXiv | [Embodied-Reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks](https://arxiv.org/abs/2503.21696) | [ğŸŒ](https://embodied-reasoner.github.io/) | [ğŸ’»](https://github.com/zwq2018/embodied_reasoner) |
| 2025 | arXiv | [Reinforced Planning: Solving long-horizon tasks via imitation and reinforcement learning](https://arxiv.org/abs/2505.22050) | - | [ğŸ’»](https://github.com/mail-taii/Reinforced-Reasoning-for-Embodied-Planning) |
| 2025 | ICRA | [Chain-of-Modality: Learning manipulation programs from multimodal human videos with vision-language-models](https://arxiv.org/abs/2504.13351) | [ğŸŒ](https://chain-of-modality.github.io/) | - |
| 2025 | arXiv | [Embodied-R: Collaborative framework for activating embodied spatial reasoning in foundation models via reinforcement learning](https://arxiv.org/abs/2504.12680) | [ğŸŒ](https://embodiedcity.github.io/Embodied-R/) | [ğŸ’»](https://github.com/EmbodiedCity/Embodied-R.code) |
| 2025 | CVPR | [RoVI: Robotic Visual Instruction](https://openaccess.thecvf.com/content/CVPR2025/papers/Li_Robotic_Visual_Instruction_CVPR_2025_paper.pdf) | [ğŸŒ](https://robotic-visual-instruction.github.io/) | - |
| 2025 | arXiv | [ReLEP: Long-horizon embodied planning with implicit logical inference and hallucination mitigation](https://arxiv.org/abs/2409.15658) | - | - |
| 2025 | CVPR | [RoboBrain: A unified brain model for robotic manipulation from abstract to concrete](https://openaccess.thecvf.com/content/CVPR2025/papers/Ji_RoboBrain_A_Unified_Brain_Model_for_Robotic_Manipulation_from_Abstract_CVPR_2025_paper.pdf) | [ğŸŒ](https://superrobobrain.github.io/) | [ğŸ’»](https://github.com/FlagOpen/RoboBrain2.0) |

---

### Planner + Policy
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2023 | arXiv | [Instruct2Act: Mapping multi-modality instructions to robotic actions with large language model](https://arxiv.org/abs/2305.11176) | - | [ğŸ’»](https://github.com/OpenGVLab/Instruct2Act) |
| 2023 | CoRL | [VoxPoser: Composable 3D value maps for robotic manipulation with language models](https://arxiv.org/abs/2307.05973) | [ğŸŒ](https://voxposer.github.io/) | [ğŸ’»](https://github.com/huangwl18/VoxPoser) |
| 2024 | CVPR | [SkillDiffuser: Interpretable hierarchical planning via skill abstractions in diffusion-based task execution](https://openaccess.thecvf.com/content/CVPR2024/papers/Liang_SkillDiffuser_Interpretable_Hierarchical_Planning_via_Skill_Abstractions_in_Diffusion-Based_Task_CVPR_2024_paper.pdf) | [ğŸŒ](https://skilldiffuser.github.io/) | [ğŸ’»](https://github.com/Liang-ZX/skilldiffuser) |
| 2024 | arXiv | [RoboMatrix: A skill-centric hierarchical framework for scalable robot task planning and execution in open-world](https://arxiv.org/abs/2412.00171) | - | [ğŸ’»](https://github.com/WayneMao/RoboMatrix) |
| 2024 | CoRL | [RT-Affordance: Reasoning about robotic manipulation with affordances](https://arxiv.org/abs/2411.02704) | [ğŸŒ](https://snasiriany.me/rt-affordance) | - |
| 2024 | CoRL | [LLARVA: Vision-action instruction tuning enhances robot learning](https://arxiv.org/abs/2406.11815) | [ğŸŒ](https://llarva24.github.io/) | [ğŸ’»](https://github.com/Dantong88/LLARVA) |
| 2024 | CVPR | [MALMM: Multi-agent large language models for zero-shot robotics manipulation](https://arxiv.org/abs/2411.17636) | [ğŸŒ](https://malmm1.github.io/) | [ğŸ’»](https://github.com/malmm1/MALMM) |
| 2024 | arXiv | [RT-H: Action Hierarchies Using Language](https://arxiv.org/abs/2403.01823) | [ğŸŒ](https://rt-hierarchy.github.io/) | - |
| 2024 | CoRL | [ReKep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation](https://arxiv.org/abs/2409.01652) | [ğŸŒ](https://rekep-robot.github.io/) | [ğŸ’»](https://github.com/huangwl18/ReKep) |
| 2025 | ICLR | [HAMSTER: Hierarchical action models for open-world robot manipulation](https://arxiv.org/abs/2503.21696) | [ğŸŒ](https://hamster-robot.github.io/) | [ğŸ’»](https://github.com/liyi14/HAMSTER_beta) |
| 2025 | ICML | [HiRobot: Open-ended instruction following with hierarchical vision-language-action models](https://arxiv.org/abs/2502.19417) | [ğŸŒ](https://www.pi.website/research/hirobot) | - |
| 2025 | arXiv | [Agentic Robot: A brain-inspired framework for vision-language-action models in embodied agents](https://arxiv.org/abs/2505.23450) | [ğŸŒ](https://agentic-robot.github.io/) | [ğŸ’»](https://github.com/Agentic-Robot/agentic-robot) |
| 2025 | arXiv | [DexVLA: Vision-language model with plug-in diffusion expert for general robot control](https://arxiv.org/abs/2502.05855) | [ğŸŒ](https://dex-vla.github.io/) | [ğŸ’»](https://github.com/juruobenruo/DexVLA) |
| 2025 | arXiv | [PointVLA: Injecting the 3D world into vision-language-action models](https://arxiv.org/abs/2503.07511) | [ğŸŒ](https://pointvla.github.io/) | - |
| 2025 | arXiv | [A0: An affordance-aware hierarchical model for general robotic manipulation](https://arxiv.org/abs/2504.12636) | [ğŸŒ](https://a-embodied.github.io/A0/) | [ğŸ’»](https://github.com/A-embodied/A0) |
| 2025 | arXiv | [From seeing to doing: Bridging reasoning and decision for robotic manipulation](https://arxiv.org/abs/2505.08548) | [ğŸŒ](https://embodied-fsd.github.io/) | [ğŸ’»](https://github.com/pickxiguapi/Embodied-FSD) |
| 2025 | ICCV | [RoBridge: A hierarchical architecture bridging cognition and execution for general robotic manipulation](https://arxiv.org/abs/2505.01709) | [ğŸŒ](https://abliao.github.io/RoBridge/) | [ğŸ’»](https://github.com/abliao/RoBridge) |
| 2025 | arXiv | [RoboCerebra: A large-scale benchmark for long-horizon robotic manipulation evaluation](https://arxiv.org/abs/2506.06677) | [ğŸŒ](https://robocerebra.github.io/) | - |
| 2025 | arXiv | [Ï€0.5: A vision-language-action model with open-world generalization](https://arxiv.org/abs/2504.16054) | [ğŸŒ](https://pi.website/blog/pi05) | - |
| 2025 | arXiv | [DexGraspVLA: A vision-language-action framework towards general dexterous grasping](https://arxiv.org/abs/2502.20900) | [ğŸŒ](https://dexgraspvla.github.io/) | [ğŸ’»](https://github.com/Psi-Robot/DexGraspVLA) |
| 2025 | arXiv | [HiBerNAC: Hierarchical brain-emulated robotic neural agent collective for disentangling complex manipulation](https://arxiv.org/abs/2506.08296) | - | - |






## Other Advanced Field

### Reinforcement Learning-based Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2025 | ICLR(Workshop) | [GRAPE: Generalizing Robot Policy via Preference Alignment](https://openreview.net/pdf?id=XnwyFD1Fvw&utm_source=chatgpt.com) | [ğŸŒ](https://grape-vla.github.io/) | [ğŸ’»](https://github.com/aiming-lab/grape) |
| 2025 | arXiv | [Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning](https://arxiv.org/abs/2505.18719) | - | [ğŸ’»](https://github.com/GuanxingLu/vlarl) |
| 2025 | RSS | [ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations](https://openreview.net/pdf?id=a6lsCozWyM) | [ğŸŒ](https://rewind-reward.github.io/) | - |
| 2025 | RSS | [ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy](https://www.roboticsproceedings.org/rss21/p019.pdf) | [ğŸŒ](https://cccedric.github.io/conrft/) | [ğŸ’»](https://github.com/cccedric/conrft) |
| 2025 | RSS | [RLDG: Robotic Generalist Policy Distillation via Reinforcement Learning](https://www.roboticsproceedings.org/rss21/p028.pdf) | [ğŸŒ](https://generalist-distillation.github.io/) | - |
| 2025 | arXiv | [TGRPO: Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization](https://arxiv.org/abs/2506.08440) | - | - |
| 2025 | ICRA | [Improving Vision-Language-Action Model with Online Reinforcement Learning](https://arxiv.org/abs/2501.16664) | - | - |
| 2025 | CVPR(Workshop) | [Interactive Postâ€‘Training for Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2505.17016) | [ğŸŒ](https://ariostgx.github.io/ript_vla/) | [ğŸ’»](https://github.com/Ariostgx/ript-vla) |

---

### Training-Free Methods
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2025 | arXiv | [VLAâ€‘Cache: Towards Efficient Visionâ€‘Languageâ€‘Action Model via Adaptive Token Caching in Robotic Manipulation](https://arxiv.org/abs/2502.02175) | - | [ğŸ’»](https://github.com/siyuhsu/vla-cache) |
| 2025 | arXiv | [Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding](https://arxiv.org/abs/2503.02310) | - | - |
| 2025 | arXiv | [Think Twice, Act Once: Tokenâ€‘Aware Compression and Action Reuse for Efficient Inference in Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2505.21200) | - | - |
| 2025 | arXiv | [EfficientVLA: Trainingâ€‘Free Acceleration and Compression for Visionâ€‘Languageâ€‘Action Models](https://arxiv.org/abs/2506.10100) | - | - |
| 2025 | arXiv | [SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration](https://arxiv.org/abs/2506.12723) | - | - |
| 2025 | arXiv | [Block-wise Adaptive Caching for Accelerating Diffusion Policy](https://arxiv.org/abs/2506.13456) | - | - |
| 2025 | RSS | [FAST: Efficient Action Tokenization for Vision-Language-Action Models](https://www.roboticsproceedings.org/rss21/p012.pdf) | [ğŸŒ](https://www.pi.website/research/fast) | - |
| 2025 | arXiv | [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339) | [ğŸŒ](https://www.pi.website/research/real_time_chunking) | - |
---

### Learning from Human Videos
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2024 | ICML | [3Dâ€‘VLA: AÂ 3D Visionâ€‘Languageâ€‘Action Generative World Model](https://arxiv.org/pdf/2403.09631) | [ğŸŒ](https://vis-www.cs.umass.edu/3dvla/) | [ğŸ’»](https://github.com/UMass-Foundation-Model/3D-VLA) |
| 2024 | NeurIPS | [Learning an Actionable Discrete Diffusion Policy via Largeâ€‘Scale Actionless Video Preâ€‘Training](https://proceedings.neurips.cc/paper_files/paper/2024/file/378226e5df7eded3e401de5c9493143c-Paper-Conference.pdf) | [ğŸŒ](https://video-diff.github.io/) | [ğŸ’»](https://github.com/tinnerhrhe/VPDD) |
| 2025 | CVPR | [Mitigating the Humanâ€‘Robot Domain Discrepancy in Visual Preâ€‘training for Robotic Manipulation](https://cvpr.thecvf.com/virtual/2025/poster/32537) | [ğŸŒ](https://jiaming-zhou.github.io/projects/HumanRobotAlign) | [ğŸ’»](https://github.com/jiaming-zhou/HumanRobotAlign) |
| 2025 | RSS | [UniVLA: Learning to Act Anywhere with Taskâ€‘centric Latent Actions](https://www.roboticsproceedings.org/rss21/p014.pdf) | - | [ğŸ’»](https://github.com/OpenDriveLab/UniVLA) |
| 2025 | ICLR | [Latent Action Pretraining from Videos](https://openreview.net/pdf?id=VYOe2eBQeh) | [ğŸŒ](https://latentactionpretraining.github.io/) | [ğŸ’»](https://github.com/LatentActionPretraining/LAPA) |
| 2025 | arXiv | [Humanoidâ€‘VLA: Towards Universal Humanoid Control with Visual Integration](https://arxiv.org/abs/2502.14795) | - | - |
<!-- | 2024 | ICML | [3Dâ€‘VLA: AÂ 3D Visionâ€‘Languageâ€‘Action Generative World Model](https://proceedings.mlr.press/v235/zhen24a.html) | [ğŸŒ](https://vis-www.cs.umass.edu/3dvla/) | [ğŸ’»](https://github.com/UMass-Foundation-Model/3D-VLA) | -->
---

### World Model-based VLA
| Year | Venue | Paper | Website | Code |
|------|-------|-------|---------|------|
| 2024 | CVPR | [FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects](https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf) | [ğŸŒ](https://nvlabs.github.io/FoundationPose/) | [ğŸ’»](https://github.com/NVlabs/FoundationPose) |
| 2024 | ICML | [3Dâ€‘VLA: AÂ 3D Visionâ€‘Languageâ€‘Action Generative World Model](https://arxiv.org/pdf/2403.09631) | [ğŸŒ](https://vis-www.cs.umass.edu/3dvla/) | [ğŸ’»](https://github.com/UMass-Foundation-Model/3D-VLA) | 
| 2025 | arXiv | [WorldVLA: Towards Autoregressive Action World Model](https://arxiv.org/abs/2506.21539) | - | [ğŸ’»](https://github.com/alibaba-damo-academy/WorldVLA) |
| 2025 | arXiv | [World4Omni: A Zeroâ€‘Shot Framework from Image Generation World Model to Robotic Manipulation](https://arxiv.org/abs/2506.23919) | [ğŸŒ](https://world4omni.github.io/) | - |
| 2025 | arXiv | [Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations](https://arxiv.org/abs/2507.00990) | [ğŸŒ](https://rigvid-robot.github.io/) | [ğŸ’»](https://github.com/shivanshpatel35/rigvid) |
| 2025 | arXiv | [Vâ€‘JEPAÂ 2: Selfâ€‘Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985) | [ğŸŒ](https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/) | [ğŸ’»](https://github.com/facebookresearch/vjepa2) |


## Datasets and Benchmarks

### Real-world Robot Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2021 | CoRL  | [BC-Z: Zero-shot task generalization with robotic imitation learning](https://proceedings.mlr.press/v164/jang22a/jang22a.pdf) | [ğŸŒ](https://sites.google.com/view/bc-z/home) | [ğŸ’»](https://github.com/google-research/tensor2robot/tree/master/research/bcz) | [ğŸ“¦](https://www.kaggle.com/datasets/google/bc-z-robot) |
| 2023 | RSS  | [RTâ€‘1:â€¯Robotics Transformer for Realâ€‘World Control at Scale](https://arxiv.org/abs/2212.06817) | [ğŸŒ](https://robotics-transformer1.github.io/) | [ğŸ’»](https://github.com/google-research/robotics_transformer) | - |
| 2023 | CoRL  | [RTâ€‘2: Visionâ€‘Language Foundation Models as Effective Robot Imitators](https://arxiv.org/abs/2307.15818) | [ğŸŒ](https://robotics-transformer2.github.io/) | [ğŸ’»](https://github.com/google-research/robotics_transformer) | - |
| 2022 | RSS  | [Bridge Data: Boosting Generalization of Robotic Skills with Crossâ€‘Domain Datasets](https://arxiv.org/abs/2109.13396) | [ğŸŒ](https://sites.google.com/view/bridgedata) | [ğŸ’»](https://github.com/yanlai00/bridge_data_imitation_learning) | [ğŸ“¦](https://sites.google.com/view/bridgedata) |
| 2023 | CoRL  | [BridgeDataâ€¯V2: A Dataset for Robot Learning at Scale](https://arxiv.org/abs/2308.12952) | [ğŸŒ](https://rail-berkeley.github.io/bridgedata/) | [ğŸ’»](https://github.com/rail-berkeley/bridge_data_v2) | [ğŸ“¦](https://github.com/rail-berkeley/bridge_data_v2) |
| 2024 | ICRA | [RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in Oneâ€‘Shot](https://arxiv.org/abs/2307.00595) | [ğŸŒ](https://rh20t.github.io/) | [ğŸ’»](https://github.com/rh20t) | [ğŸ“¦](https://github.com/rh20t/act_baseline) |
| 2024 | RSS | [DROID: A Large-Scale Inâ€‘Theâ€‘Wild Robot Manipulation Dataset](https://arxiv.org/abs/2403.12945) | [ğŸŒ](https://droid-dataset.github.io/) | [ğŸ’»](https://github.com/droid-dataset/droid) | [ğŸ“¦](https://github.com/droid-dataset/droid_policy_learning) |
| 2024 | ICRA | [Openâ€¯Xâ€‘Embodiment: Robotic Learning Datasets and RTâ€‘X Models](https://arxiv.org/abs/2310.08864) | [ğŸŒ](https://robotics-transformer-x.github.io/) | [ğŸ’»](https://github.com/google-deepmind/open_x_embodiment) | [ğŸ“¦](https://github.com/google-deepmind/open_x_embodiment) |
| 2025 | RSS | [RoboMIND: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation](https://arxiv.org/abs/2412.13877) | [ğŸŒ](https://x-humanoid-robomind.github.io/) | [ğŸ’»](https://github.com/x-humanoid-robomind/x-humanoid-robomind.github.io) | [ğŸ“¦](https://data.flopsera.com/data-detail/21181956226031626?type=open) |
| 2025 | IROS | [AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems](https://arxiv.org/abs/2503.06669) | [ğŸŒ](https://agibot-world.com/) | [ğŸ’»](https://github.com/OpenDriveLab/AgiBot-World) | [ğŸ“¦](https://huggingface.co/datasets/agibot-world/AgiBotWorld-Alpha) |
| 2025 | arXiv | [BRMData: Empowering Embodied Manipulation: A Bimanual-Mobile Robot Manipulation Dataset for Household Tasks](https://arxiv.org/pdf/2405.18860) | [ğŸŒ](https://embodiedrobot.github.io/) | [ğŸ’»](https://github.com/Louis-ZhangLe/BRMData) | [ğŸ“¦](http://box.jd.com/sharedInfo/1147DC284DDAEE91DC759E209F58DD60) |

---

### Simulation Environments and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2022 | CoRL | [BEHAVIORâ€‘1K: A Humanâ€‘Centered, Embodied AI Benchmark with 1,000 Everyday Activities and Realistic Simulation](https://arxiv.org/abs/2403.09227) | [ğŸŒ](https://behavior.stanford.edu/) | [ğŸ’»](https://github.com/StanfordVL/BEHAVIOR-1K) | [ğŸ“¦](https://github.com/StanfordVL/BEHAVIOR-1K) |
| 2020 | CVPR | [ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks](https://arxiv.org/abs/1912.01734) | [ğŸŒ](https://askforalfred.com/) | [ğŸ’»](https://github.com/askforalfred/alfred) | [ğŸ“¦](https://askforalfred.com/) |
| 2020 | RA-L | [RLBench: The Robot Learning Benchmark & Learning Environment](https://arxiv.org/abs/1909.12271) | [ğŸŒ](https://sites.google.com/view/rlbench) | [ğŸ’»](https://github.com/stepjam/RLBench) | [ğŸ“¦](https://huggingface.co/datasets/hqfang/RLBench-18-Tasks) |
| 2024 | arXiv | [PerActÂ²: Benchmarking and Learning for Robotic Bimanual Manipulation Tasks](https://arxiv.org/abs/2407.00278) | [ğŸŒ](http://bimanual.github.io/) | [ğŸ’»](https://github.com/markusgrotz/peract_bimanual) | [ğŸ“¦](https://bimanual.github.io/) |
| 2020 | CoRL | [Metaâ€‘World: A Benchmark and Evaluation for Multiâ€‘Task and Meta Reinforcement Learning](https://arxiv.org/abs/1910.10897) | [ğŸŒ](https://meta-world.github.io/) | [ğŸ’»](https://github.com/Farama-Foundation/Metaworld) | [ğŸ“¦](https://github.com/Farama-Foundation/Metaworld) |
| 2019 | CoRL | [Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning](https://arxiv.org/abs/1910.11956) | [ğŸŒ](https://relay-policy-learning.github.io/) | [ğŸ’»](https://github.com/google-research/relay-policy-learning) | [ğŸ“¦](https://minari.farama.org/datasets/D4RL/index.html) |
| 2023 | NeurIPS | [LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning](https://arxiv.org/abs/2306.03310) | [ğŸŒ](https://libero-project.github.io/) | [ğŸ’»](https://github.com/Lifelong%E2%80%91Robot%E2%80%91Learning/LIBERO) | [ğŸ“¦](https://github.com/Lifelong%E2%80%91Robot%E2%80%91Learning/LIBERO) |
| 2022 | RA-L | [CALVIN: A Benchmark for Languageâ€‘Conditioned Policy Learning for Longâ€‘Horizon Robot Manipulation Tasks](https://arxiv.org/abs/2112.03227) | [ğŸŒ](http://calvin.cs.uni-freiburg.de/) | [ğŸ’»](https://github.com/mees/calvin) | [ğŸ“¦](https://github.com/mees/calvin) |
| 2024 | arXiv | [MIKASA: Memory, Benchmark & Robots: A Benchmark for Solving Complex Tasks with Reinforcement Learning](https://arxiv.org/abs/2502.10550) | [ğŸŒ](https://sites.google.com/view/memorybenchrobots/) | [ğŸ’»](https://github.com/CognitiveAISystems/MIKASA-Robo) | [ğŸ“¦](https://huggingface.co/datasets/avanturist/mikasa-robo) |
| 2024 | CoRL | [SIMPLER: Evaluating Realâ€‘World Robot Manipulation Policies in Simulation](https://arxiv.org/abs/2405.05941) | [ğŸŒ](https://simpler-env.github.io/) | [ğŸ’»](https://github.com/simpler-env/SimplerEnv) | [ğŸ“¦](https://github.com/simpler-env/SimplerEnv) |
| 2019 | ICCV | [Habitat: A Platform for Embodied AI Research](https://arxiv.org/abs/1904.01201) | [ğŸŒ](https://aihabitat.org/) | [ğŸ’»](https://github.com/facebookresearch/habitat-sim) | [ğŸ“¦](https://aihabitat.org/datasets/replica_cad/) |
| 2021 | NeurIPS| [Habitatâ€¯2.0: Training Home Assistants to Rearrange their Habitat](https://arxiv.org/abs/2106.14405) | [ğŸŒ](https://aihabitat.org/docs/habitat2) | [ğŸ’»](https://github.com/facebookresearch/habitat-sim) | [ğŸ“¦](https://aihabitat.org/datasets/replica_cad/) |
| 2024 | ICLR | [Habitatâ€¯3.0: A Coâ€‘Habitat for Humans, Avatars and Robots](https://arxiv.org/abs/2310.13724) | [ğŸŒ](https://aihabitat.org/habitat3/) | [ğŸ’»](https://github.com/facebookresearch/habitat-sim) | [ğŸ“¦](https://aihabitat.org/datasets/replica_cad/) |
| 2020 | CVPR | [SAPIEN: A Simulated Part-based Interactive Environment](https://arxiv.org/abs/2003.08515) | [ğŸŒ](https://sapien.ucsd.edu/) | [ğŸ’»](https://github.com/haosulab/SAPIEN) | [ğŸ“¦](https://sapien.ucsd.edu/downloads) |
| 2024 | RSS | [Theâ€¯Colosseum: A Benchmark for Evaluating Generalization for Robotic Manipulation](https://arxiv.org/abs/2402.08191) | [ğŸŒ](https://robot-colosseum.github.io/) | [ğŸ’»](https://github.com/robot-colosseum/robot-colosseum) | [ğŸ“¦](https://huggingface.co/datasets/colosseum/colosseum-challenge) |
| 2025 | ICCV | [VLABench: A Largeâ€‘Scale Benchmark for Languageâ€‘Conditioned Robotics Manipulation with Longâ€‘Horizon Reasoning Tasks](https://arxiv.org/abs/2412.18194) | [ğŸŒ](https://vlabench.github.io/) | [ğŸ’»](https://github.com/OpenMOSS/VLABench) | [ğŸ“¦](https://huggingface.co/VLABench) |

---

### Human Behavior Datasets
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2022 | CVPR | [Ego4D: Around the World in 3,000 Hours of Egocentric Video](https://arxiv.org/abs/2110.07058) | [ğŸŒ](https://ego4d-data.org/) | [ğŸ’»](https://github.com/facebookresearch/Ego4d) | [ğŸ“¦](https://ego4d-data.org/docs/start-here/) |
| 2024 | CVPR | [Egoâ€‘Exo4D: Understanding Skilled Human Activity from Firstâ€‘ and Thirdâ€‘Person Perspectives](https://arxiv.org/abs/2311.18259) | [ğŸŒ](https://ego-exo4d-data.org/) | [ğŸ’»](https://github.com/facebookresearch/Ego4d) | [ğŸ“¦](https://ego-exo4d-data.org/) |
| 2024 | arXiv | [EgoPlanâ€‘Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models](https://arxiv.org/abs/2312.06722) | [ğŸŒ](https://chenyi99.github.io/ego_plan/) | [ğŸ’»](https://github.com/ChenYi99/EgoPlan) | [ğŸ“¦](https://drive.google.com/drive/folders/1qVtPzhHmCgdQ5JlMeAL3OZtvbHaXktTo) |
| 2024 | arXiv | [EgoVidâ€‘5M: A Largeâ€‘Scale Videoâ€‘Action Dataset for Egocentric Video Generation](https://arxiv.org/abs/2411.08380) | [ğŸŒ](https://egovid.github.io/) | [ğŸ’»](https://github.com/JeffWang987/EgoVid) | [ğŸ“¦](https://modelscope.cn/datasets/iic/EgoVid/) |
| 2018 | ECCV | [Scaling Egocentric Vision: The EPICâ€‘KITCHENS Dataset](https://arxiv.org/abs/1804.02748) | [ğŸŒ](http://epic-kitchens.github.io/) | [ğŸ’»](http://epic-kitchens.github.io/) | [ğŸ“¦](https://epic-kitchens.github.io/2025#downloads) |
| 2024 | ECCV| [COMâ€¯Kitchens: An Unedited Overheadâ€‘View Video Dataset as a Visionâ€‘Language Benchmark](https://arxiv.org/abs/2408.02272) | [ğŸŒ](https://github.com/omron-sinicx/com_kitchens) | [ğŸ’»](https://github.com/omron-sinicx/com_kitchens) | [ğŸ“¦](https://github.com/omron-sinicx/com_kitchens) |
| 2019 | ICCV | [EgoVQA: An Egocentric Video Question Answering Benchmark Dataset](https://openaccess.thecvf.com/content_ICCVW_2019/papers/EPIC/Fan_EgoVQA_-_An_Egocentric_Video_Question_Answering_Benchmark_Dataset_ICCVW_2019_paper.pdf) | [ğŸŒ](https://github.com/YaoMarkMu/EgoVQA) | [ğŸ’»](https://github.com/YaoMarkMu/EgoVQA) | [ğŸ“¦](http://vision.soic.indiana.edu/identifying-1st-3rd/) |
| 2022 | NeurIPS | [EgoTaskQA: Understanding Human Tasks in Egocentric Videos](https://arxiv.org/abs/2210.03929) | [ğŸŒ](https://sites.google.com/view/egotaskqa) | [ğŸ’»](https://github.com/Buzz-Beater/EgoTaskQA) | [ğŸ“¦](https://sites.google.com/view/egotaskqa) |
| 2025 | arXiv| [EgoDex: Learning Dexterous Manipulation from Largeâ€‘Scale Egocentric Video](https://arxiv.org/abs/2505.11709) | - | - | - |
| 2024 | RSS | [DexCap: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation](https://arxiv.org/abs/2403.07788) | [ğŸŒ](https://dex-cap.github.io/) | [ğŸ’»](https://github.com/j96w/DexCap) | [ğŸ“¦](https://huggingface.co/datasets/chenwangj/DexCap-Data) |
| 2024 | arXiv | [EgoMimic: Scaling Imitation Learning via Egocentric Video](https://arxiv.org/abs/2410.24221) | [ğŸŒ](https://egomimic.github.io/) | [ğŸ’»](https://github.com/SimarKareer/EgoMimic-Eve) | [ğŸ“¦](https://huggingface.co/datasets/gatech/EgoMimic/tree/main) |
| 2025 | CoRL | [Humanoid Policy ~ Human Policy](https://arxiv.org/abs/2503.13441) | [ğŸŒ](https://human-as-robot.github.io/) | [ğŸ’»](https://github.com/RogerQi/human-policy) | [ğŸ“¦](https://huggingface.co/datasets/RogerQi/PH2D) |
| 2025 | arXiv | [Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware](https://arxiv.org/abs/2505.09601) | [ğŸŒ](https://real2render2real.com/) | [ğŸ’»](https://github.com/uynitsuj/real2render2real) | - |

---

### Embodied Datasets and Benchmarks
| Year | Venue | Paper | Website | Code | Data |
|------|-------|-------|---------|------|------|
| 2018 | CVPR | [EQA: Embodied Question Answering](https://arxiv.org/abs/1711.11543) | [ğŸŒ](https://embodiedqa.org/) | [ğŸ’»](https://github.com/facebookresearch/EmbodiedQA) | [ğŸ“¦](https://embodiedqa.org/data) |
| 2018 | CVPR | [IQA: Visual Question Answering in Interactive Environments](https://arxiv.org/abs/1712.03316) | - | [ğŸ’»](https://github.com/danielgordon10/thor-iqa-cvpr-2018) | - |
| 2019 | CVPR | [MTâ€‘EQA: Multiâ€‘Target Embodied Question Answering](https://arxiv.org/abs/1904.04686) | [ğŸŒ](https://embodiedqa.org/) | [ğŸ’»](https://github.com/facebookresearch/EmbodiedQA) | [ğŸ“¦](https://embodiedqa.org/data) |
| 2019 | CVPR | [Embodied Question Answering in Photorealistic Environments with Point Cloud Perception](https://arxiv.org/abs/1904.03461) | [ğŸŒ](https://embodiedqa.org/) | [ğŸ’»](https://github.com/facebookresearch/EmbodiedQA) | [ğŸ“¦](https://embodiedqa.org/data) |
| 2023 | ICLR | [EQAâ€‘MX: Embodied Question Answering using Multimodal Expression](https://openreview.net/pdf) | - | - | - |
| 2024 | CVPR | [OpenEQA: Embodied Question Answering in the Era of Foundation Models](https://openaccess.thecvf.com/content/CVPR2024/papers/Majumdar_OpenEQA_Embodied_Question_Answering_in_the_Era_of_Foundation_Models_CVPR_2024_paper.pdf) | [ğŸŒ](https://open-eqa.github.io/) | [ğŸ’»](https://github.com/facebookresearch/open-eqa) | [ğŸ“¦](https://github.com/facebookresearch/open-eqa) |
| 2024 | ICLR | [LoTaâ€‘Bench: Benchmarking Languageâ€‘oriented Task Planners for Embodied Agents](https://arxiv.org/abs/2402.08178) | [ğŸŒ](https://choi-jaewoo.github.io/LoTa-Bench/) | [ğŸ’»](https://github.com/lbaa2022/LLMTaskPlanning) | [ğŸ“¦](https://github.com/lbaa2022/LLMTaskPlanning) |






